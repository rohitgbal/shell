<?xml version="1.0" encoding="UTF-8"?>
<workflow-app xmlns="uri:oozie:workflow:0.5" name="prg1${project}_${workflow}_WF">
   
   <global>
		<job-tracker>${job_tracker}</job-tracker>
		<name-node>${name_node}</name-node>
		<configuration>
			<property>
				<name>mapreduce.job.queuename</name>
				<value>${oozie_queue_name}</value>
			</property>
			<property>
				<name>oozie.launcher.mapreduce.job.user.classpath.first</name>
				<value>true</value>
			</property>
			<property>
				<name>oozie.launcher.mapreduce.task.classpath.user.precedence</name>
				<value>true</value>
			</property>
		</configuration>
	</global>
	
	<credentials>
		<credential name="hiveCredentials" type="hive2">
			<property>
				<name>hive2.jdbc.url</name>
				<value>${hive_beeline_server}</value>
			</property>
			<property>
				<name>hive2.server.principal</name>
				<value>${hive_kerberos_principal}</value>
			</property>
		</credential>
		<credential name="hbaseCredentials" type="hbase">
		</credential>
	</credentials>
	
    <start to="CleanUp_AXT_Staging_Table" />
    
    <action name="CleanUp_AXT_Staging_Table"
		cred="hiveCredentials">
		<hive2 xmlns="uri:oozie:hive2-action:0.1">
			<prepare>
				<delete path="${output}"/>
				<mkdir path="${output}"/>
			</prepare>
			<jdbc-url>${hive_beeline_server}</jdbc-url>
			<script>${path_hdfs_workflow_root}/drop_staging_partitions.ql
			</script>
			<param>db_name=${hive_database_name}</param>
		</hive2>
		<ok to="Get_CRC4T_NA_Data" />
		<error to="Failure_Notification" />
	</action>
    
	<action name="Get_CRC4T_NA_Data">
        <shell xmlns="uri:oozie:shell-action:0.3">
			<exec>run4G_History_Reload.sh</exec>
			<argument>${spark_master}</argument>
			<argument>${spark_deploy_mode}</argument>
			<argument>${spark_max_executors}</argument>
			<argument>${spark_driver_memory}</argument>
			<argument>${spark_executor_memory}</argument>
			<argument>${spark_executor_cores}</argument>
			<argument>${output}</argument>
			<argument>${hive_database_name}</argument>
			<argument>${kerberos_keytab_filename}</argument>
			<argument>${kerberos_principal}</argument>
            <argument>${current_user}</argument>
            <argument>${lkb_days}</argument>
            <argument>${NA_source_table}</argument>
			<argument>${current_date}</argument>
			<file>${path_hdfs_project}/run4G.sh#run4G_History_Reload.sh</file>
			<file>${path_hdfs_project}/loader.py#loader.py</file>
			<file>${path_hdfs_project}/4gscript_History_Reload.sql#4gscript_History_Reload.sql</file>
			<file>${kerberos_keytab_file}#${kerberos_keytab_filename}</file>
		</shell>
		<ok to="Get_CRC4T_EU_Data" />
		<error to="Capture_Failure_reccalc" />
    </action>

	<action name="Get_CRC4T_EU_Data">
        <shell xmlns="uri:oozie:shell-action:0.3">
			<exec>run4G.sh</exec>	
			<argument>${spark_master}</argument>
			<argument>${spark_deploy_mode}</argument>
			<argument>${spark_max_executors}</argument>
			<argument>${spark_driver_memory}</argument>
			<argument>${spark_executor_memory}</argument>
			<argument>${spark_executor_cores}</argument>
			<argument>${output}</argument>
			<argument>${hive_database_name}</argument>
			<argument>${kerberos_keytab_filename}</argument>
			<argument>${kerberos_principal}</argument>
            <argument>${current_user}</argument>
            <argument>${lkb_days}</argument>	
            <argument>${eu_source_table}</argument>
			<argument>${current_date}</argument>
			<file>${path_hdfs_project}/run4G.sh#run4G.sh</file>
			<file>${path_hdfs_project}/loader.py#loader.py</file>
			<file>${path_hdfs_project}/4gscript_History_Reload.sql#4gscript_History_Reload.sql</file>
			<file>${kerberos_keytab_file}#${kerberos_keytab_filename}</file>
		</shell>
		<ok to="Load_To_Master" />
		<error to="Capture_Failure_reccalc" />
    </action>
	
	<action name="Load_To_Master" cred="hiveCredentials">
                <hive2 xmlns="uri:oozie:hive2-action:0.1">
                        <jdbc-url>${hive_beeline_server}</jdbc-url>
                        <script>${path_hdfs_project}/loadToMaster.ql</script>
			<param>db_name=${hive_database_name}</param>
			<param>lkb_days=${lkb_days}</param>
			<param>current_date=${current_date}</param>
                </hive2>
                <ok to="Load_VHA" />
		        <error to="Failure_Notification" />
        </action>
		
	<action name="Load_VHA" cred="hiveCredentials">
                <hive2 xmlns="uri:oozie:hive2-action:0.1">
                        <jdbc-url>${hive_beeline_server}</jdbc-url>
                        <script>${path_hdfs_project}/VHAscript.ql</script>
			<param>db_name=${hive_database_name}</param>
			<param>lkb_days=${lkb_days}</param>
			<param>current_date=${current_date}</param>
                </hive2>
                <ok to="Capture_Success_reccalc" />
		        <error to="Failure_Notification" />
        </action>
	
	<action name="Capture_Success_reccalc" cred="hiveCredentials">
                <hive2 xmlns="uri:oozie:hive2-action:0.1">
                        <jdbc-url>${hive_beeline_server}</jdbc-url>
                        <script>${path_hdfs_project}/success_reccalc.ql</script>
						<param>db_name=${hive_database_name}</param>
                        <param>current_user=${kerberos_user_name}</param>
						<param>workflowId=${wf:id()}</param>
						<param>workflowName=${workflow}</param>
						<param>mstr_tbl=${mstr_tbl}</param>
						<param>reccalc_file_type=${reccalc_file_type}</param>
                </hive2>
                <ok to="end" />
		        <error to="Failure_Notification" />
        </action>
	
	<action name="Capture_Failure_reccalc">
		<java>
			<main-class>${reccalc_driver_class}</main-class>
			<java-opts>-Dsun.security.krb5.debug=true</java-opts>
			<arg>reccalcClass=${reccalc_calculator_class}</arg>
  			<arg>fileType=${reccalc_file_type}</arg>
  			<arg>workflowId=${wf:id()}</arg>
  			<arg>workflowName=${workflow}</arg>
  			<arg>userName=${current_user}</arg>
  			<arg>workflowFailure=True</arg>
  			<arg>failedStep=${wf:lastErrorNode()}</arg>
			<file>${path_custom_jar_lib}/prg1Common.jar#prg1Common.jar</file>
			<file>${path_custom_jar_lib}/lib/itcore.jar#itcore.jar</file>
			<file>${environment_hdfs_folder}/Workflow/KEYS/prg1_jaas.conf#prg1_jaas.conf</file>
			<file>${kerberos_keytab_file}#${kerberos_keytab_filename}</file>	                 
		</java>
		<ok to="Failure_Notification" />
		<error to="Failure_Notification" />
	</action>
	
	<action name="Failure_Notification">
		<email xmlns="uri:oozie:email-action:0.1">
			<to>${email_recipient}</to>
			<subject>${email_subject_failure} ${wf:id()}</subject>
			<body>The workflow prg1${project}_${workflow}_WF (${wf:id()}) 
				Failed in the ${wf:lastErrorNode()} process.
				
				Please check the following logs for further details:
				From Internet Explorer (Hadoop Hue): ${log_viewer1}/${wf:id()}// (Manually add the last '/' on the link above)

				From FireFox (Hadoop Logs) with use of MIT Kerberos Ticket Manager:
				${log_viewer2}/${wf:id()}?show=graph
				${log_viewer3}/${wf:id()}

				For more information on how to set-up MIT Kerberos for Windows Ticket
				Manager:
				click on http://www.NND.prdx.com/help/hadoop/odbc.html				
			</body>
		</email>
		<ok to="fail"/>
		<error to="fail"/>
	</action>
  
  <kill name="fail">
    <message>Script failed, error message[${wf:errorMessage(wf:lastErrorNode())}]</message>
  </kill>

  <end name='end' />
  
</workflow-app>
