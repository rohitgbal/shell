#Application Configuration (prg1)
application=prg1
oozie_queue_name=prg1
path_hdfs_prg1_root=/project/prg1

#Job Configuration 
project=dataprogRepository
workflow=LoadMKD_SettingsSummary_PJ1
reccalcFileType=dataprogRepository-MKD_SettingsSummary_PJ1

#Envrionment Configuration (Dev/QA/Prod)
current_user=prg1devp
current_date=2020-06-15
environment_hdfs_folder=${path_hdfs_prg1_root}/prg1DEV
hive_database_name=prg1
hbase_table_namespace=prg1_dev

#Data Files/Tables Path and Name Configuration
master_table=nscvp87_tcx_setg_pj1_pii_hte
source_table=ntblx01_CRC4T_msg_pj1_sec_hte
staging_table=nscvs87_tcx_setg_pj1_st_hte

#HDFS Paths, Folders and File Names
path_hdfs_common_path=${environment_hdfs_folder}/Workflow/Common/Workflows/Common
path_hdfs_workflow_root=${environment_hdfs_folder}/Workflow/${project}/Workflows/${workflow}
path_hdfs_staging=${environment_hdfs_folder}/Warehouse/Staging/${project}
path_hdfs_secure=${environment_hdfs_folder}/Warehouse/Secure/${project}
path_hdfs_prg1_lib=${environment_hdfs_folder}/Workflow/lib
common_load_sucess_reccalc_script=Common_Load_Success_reccalc.ql
reccalc_col=vin
where=where 1 = 1
output_staging_hdfs_path = ${path_hdfs_staging}/${staging_table}

#reccalc Configuration
reccalc_driver_class=com.prdx.it.prg1.driver.reccalcDriver
reccalc_calculator_class=com.prdx.it.prg1.process.reccalc.ReceiveOnlyNoCountreccalcCalculator

#Workflow Email Configuration
email_recipient=XXXX@XXXX.COM
email_subject_failure=prg1 QA: Workflow prg1dataprogRepository_LoadMKD_SettingsSummary_PJ1 Failed

#Workflow Schedule Configuration
frequency_interval=1440
start_date_time_in_utc=2020-06-06T00:00Z
end_date_time_in_utc=2030-04-01T00:00Z

#Hive Configuration
hive_beeline_server=jdbc:hive2://NNDhdd2-zk-1.NND.prdx.com:2181,NNDhdd2-zk-2.NND.prdx.com:2181,NNDhdd2-zk-3.NND.prdx.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-apps
hive_kerberos_principal=hive/XXXX@XXXX.COM

#Cluster Configuration
name_node=hdfs://NNDhdd2.NND.prdx.com:8020
job_tracker=NNDhdd2.NND.prdx.com:8050
log_viewer1=https://NNDhdd2e.NND.prdx.com:8000/oozie/list_oozie_workflow
log_viewer2=http://NNDhdd2.NND.prdx.com:11000/oozie/v1/job
log_viewer3=http://NNDhdd2.NND.prdx.com:11000/oozie?job=
workflow_path=${path_hdfs_workflow_root}/workflow.xml
oozie_url=http://NNDhdd2.NND.prdx.com:11000/oozie

#Oozie Configuration
oozie.wf.application.path=${name_node}${path_hdfs_workflow_root}
oozie.libpath=${path_hdfs_workflow_root},${path_hdfs_prg1_lib}/lib,${path_hdfs_prg1_lib}/prg1Common.jar
oozie.use.system.libpath=true
oozie.launcher.mapreduce.task.classpath.user.precedence=true;
oozie.launcher.mapreduce.user.classpath.first=true;
oozie.launcher.mapreduce.job.user.classpath.first=true;

#Kerberos Configuration
kerberos_user_name=${user.name}
kerberos_principal=${user.name}@NND.prdx.COM
kerberos_keytab_filename=${user.name}.keytab
kerberos_keytab_file=${environment_hdfs_folder}/Workflow/KEYS/${kerberos_keytab_filename}

#Query days
MKD_settings_query_days=30

path_hdfs_jaas_file=${environment_hdfs_folder}/Workflow/KEYS/prg1_jaas.conf

#HWC-Jars#
hive_jars=/usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly*.jar
py_spark_warehouse=/usr/hdp/current/hive_warehouse_connector/pyspark_hwc*.zip


# Spark Python Configuration
###########################################################################
spark_jars=/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-4.2.1.jar,/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-4.1.7.jar,/usr/hdp/current/spark2-client/jars/datanucleus-core-4.1.6.jar
spark_master=yarn
spark_deploy_mode=cluster
spark_files=/usr/hdp/current/spark2-client/conf/hive-site.xml
spark_network_timeout=800
spark_ui_port=5052
spark_driver_memory=4g
spark_num_executors=25
spark_executor_memory=4g
spark_executor_cores=2
spark_shuffle_memoryFraction=1
spark_shuffle_compress=true
spark_sql_shuffle_partitions=300
spark_sql_orc_filterPushdown=true
spark_yarn_executor_memoryOverhead=4096
spark_core_connection_ack_wait_timeout=600
spark_task_maxfailures=20
spark_shuffle_io_maxRetries=20
spark_dynamicalLocation_enabled=false
spark_sql_orc_enabled=true
spark_sql_hive_convertMetaStoreOrc=true
spark_sql_orc_char_enabled=true
spark_hadoop_metastore_catalog_default=hive
llap_conecter_jar=/opt/cloudera/parcels/CDH-7.1.6-1.cdh7.1.6.p6.12486751/jars/hive-warehouse-connector-assembly-*.jar
llap_conecter_zip=/opt/cloudera/parcels/CDH-7.1.6-1.cdh7.1.6.p6.12486751/lib/hive_warehouse_connector/pyspark_hwc-*.zip