#Application Configuration (prg1)
application=prg1
oozie_queue_name=prg1
path_hdfs_prg1_root=/project/prg1

#Job Configuration
project=dataprogRepository
workflow=LoaddevpactHistory
reccalcFileTypeschedule=dataprogRepository-LoaddevpactHistory



#Envrionment Configuration (Dev/QA/Prod)
current_user=prg1devp
environment_hdfs_folder=${path_hdfs_prg1_root}/prg1DEV
hive_database_name=prg1
hbase_table_namespace=prg1_dev
current_date=2020-08-19
arcv_days=15
delete_class=com.prdx.it.prg1.driver.DatePartitionDeleteDriver
MKD_messages_retention=15
delete_minHour=0
delete_maxHour=1
MKD_messages_partition=created_date
archive_table_path=${environment_hdfs_folder}/Warehouse/Secure/dataprogRepository/nscvsfy_tcx_hist_archv_st_hte
archive_table=nscvsfy_tcx_hist_archv_st_hte


#Data Files/Tables Path and Name Configuration
master_table_devpact=nscvpfy_tcx_sched_hist_pii_hte
source_history_table=ntblx01_fnv2_msg_sec_hte
staging_table=nscvsfy_tcx_hist_st_hte
source_table=ntblZ01_fcproc_msg_rawdata_k2_nondup_st_hte
function_msg_name_devpact=MKDActivationScheduleStatusAlert
raw=MKD/devpact_Raw
failure=MKD/Failure

raw_input_location=${environment_hdfs_folder}/Warehouse/Staging/dataprogRepository/MKD/FNV2_Raw
staging_inprocess=MKD/Inprocess/nscvsfy_temp_tcx_hist_st_hte
raw_inprocess_location=${environment_hdfs_folder}/Warehouse/Staging/${project}/${staging_inprocess}
raw_failure_location=${environment_hdfs_folder}/Warehouse/Staging/${project}/${failure}

#HDFS Paths, Folders and File Names
path_hdfs_common_path=${environment_hdfs_folder}/Workflow/Common/Workflows/Common
path_hdfs_workflow_root=${environment_hdfs_folder}/Workflow/${project}/Workflows/${workflow}
path_hdfs_staging=${environment_hdfs_folder}/Warehouse/Staging/${project}
path_hdfs_secure=${environment_hdfs_folder}/Warehouse/Secure/${project}
path_hdfs_prg1_lib=${environment_hdfs_folder}/Workflow/lib
output_staging_hdfs_path = ${path_hdfs_staging}/${staging_table}

#reccalc Configuration
reccalc_driver_class=com.prdx.it.prg1.driver.reccalcDriver
reccalc_calculator_class=com.prdx.it.prg1.process.reccalc.ReceiveOnlyNoCountreccalcCalculator

#Workflow Email Configuration
email_recipient=XXXX@XXXX.COM
email_subject_failure=prg1 DEV: Workflow prg1dataprogRepository_LoaddevpactHistoryPJ1 Failed

#Workflow Schedule Configuration
frequency_interval=1440
start_date_time_in_utc=2020-06-06T00:00Z
end_date_time_in_utc=2030-04-01T00:00Z

#Hive Configuration
hive_beeline_server=jdbc:hive2://NNDhdq2-zk-1.NND.prdx.com:2181,NNDhdq2-zk-2.NND.prdx.com:2181,NNDhdq2-zk-3.NND.prdx.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-apps
#hive_beeline_server=jdbc:hive2://NNDhdd2-zk-1.NND.prdx.com:2181,NNDhdd2-zk-2.NND.prdx.com:2181,NNDhdd2-zk-3.NND.prdx.com:2181/;serviceDiscoveryMode=zooKeeper;zooKeeperNamespace=hiveserver2-apps
hive_kerberos_principal=hive/XXXX@XXXX.COM

#Cluster Configuration
name_node=hdfs://hdq2cluster
#name_node=hdfs://NNDhdd2.NND.prdx.com:8020
#job_tracker=NNDhdd2.NND.prdx.com:8050
#log_viewer1=https://NNDhdd2e.NND.prdx.com:8000/oozie/list_oozie_workflow
#log_viewer2=http://NNDhdd2.NND.prdx.com:11000/oozie/v1/job
#log_viewer3=http://NNDhdd2.NND.prdx.com:11000/oozie?job=
workflow_path=${path_hdfs_workflow_root}/workflow.xml
#oozie_url=http://NNDhdd2.NND.prdx.com:11000/oozie

hdfs_mass_move_class=com.prdx.it.prg1.driver.HdfsMassFileMove

#Oozie Configuration
oozie.wf.application.path=${name_node}${path_hdfs_workflow_root}
path_custom_jar_lib=${environment_hdfs_folder}/Workflow/lib
oozie.libpath=${path_hdfs_workflow_root},${path_custom_jar_lib}/lib/itcore.jar,${path_custom_jar_lib}/prg1Common.jar,${path_custom_jar_lib}/prg1dataprogRepository.jar,${path_hdfs_sharelib_hive}
oozie.use.system.libpath=true
oozie.launcher.mapreduce.task.classpath.user.precedence=true;
oozie.launcher.mapreduce.user.classpath.first=true;
oozie.launcher.mapreduce.job.user.classpath.first=true;

#Kerberos Configuration
kerberos_user_name=${user.name}
kerberos_principal=${user.name}@NND.prdx.COM
kerberos_keytab_filename=${user.name}.keytab
kerberos_keytab_file=${environment_hdfs_folder}/Workflow/KEYS/${kerberos_keytab_filename}

#Query days
Incremental=true
MKD_schedule_query_days=30

#HWC-Jars#
hive_jars=/usr/hdp/current/hive_warehouse_connector/hive-warehouse-connector-assembly*.jar
py_spark_warehouse=/usr/hdp/current/hive_warehouse_connector/pyspark_hwc*.zip
path_hdfs_jaas_file=${environment_hdfs_folder}/Workflow/KEYS/prg1_jaas.conf
#json_jar1 =/s/hadoop/user/cases/633056/hive-hcatalog-core-1.21.2.3.1.4.0-315.jar
#json_jar2 =/s/hadoop/user/cases/633056/hive-serde-1.21.2.3.1.4.0-315.jar
json_jar1=/opt/cloudera/parcels/CDH/jars/hive-hcatalog-core-3.1.3000.7.1.6.6-5.jar
json_jar2=/opt/cloudera/parcels/CDH/jars/hive-serde-3.1.3000.7.1.6.6-5.jar

# Spark Python Configuration
###########################################################################
spark_jars=/usr/hdp/current/spark2-client/jars/datanucleus-api-jdo-4.2.1.jar,/usr/hdp/current/spark2-client/jars/datanucleus-rdbms-4.1.7.jar,/usr/hdp/current/spark2-client/jars/datanucleus-core-4.1.6.jar
spark_master=yarn
spark_deploy_mode=cluster
spark_files=/usr/hdp/current/spark2-client/conf/hive-site.xml
spark_network_timeout=800
spark_ui_port=5052
spark_driver_memory=4g
spark_num_executors=25
spark_executor_memory=4g
spark_executor_cores=2
spark_shuffle_memoryFraction=1
spark_shuffle_compress=true
spark_sql_shuffle_partitions=300
spark_sql_orc_filterPushdown=true
spark_yarn_executor_memoryOverhead=4096
spark_core_connection_ack_wait_timeout=600
spark_task_maxfailures=20
spark_shuffle_io_maxRetries=20
spark_dynamicalLocation_enabled=false
spark_sql_orc_enabled=true
spark_sql_hive_convertMetaStoreOrc=true
spark_sql_orc_char_enabled=true
spark_hadoop_metastore_catalog_default=hive